{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import pickle\n",
    "import glob\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cm(actual, pred):\n",
    "    print(pd.crosstab(pd.Series(actual.astype('int').flatten()), pd.Series(pred.astype('int').flatten()), rownames=['True'], colnames=['Predicted'], margins=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score,f1_score,precision_score,recall_score\n",
    "def rmse(predict,actual):\n",
    "    predict = np.maximum(predict,0)\n",
    "    return np.sqrt(np.mean((actual-predict)**2))\n",
    "\n",
    "def mad(predict,actual):\n",
    "    predict = np.maximum(predict,0)\n",
    "    return np.mean(np.abs(actual-predict))\n",
    "\n",
    "def prmse(predict,actual):\n",
    "    idx = np.where(actual > 0.05)\n",
    "    predict = np.maximum(predict,0)\n",
    "    return np.sqrt(np.mean((actual[idx]-predict[idx])**2))\n",
    "\n",
    "def pmad(predict,actual):\n",
    "    idx = np.where(actual > 0.05)\n",
    "    predict = np.maximum(predict,0)\n",
    "    return np.mean(np.abs(actual[idx]-predict[idx]))\n",
    "\n",
    "def classification_metrics(predicted,actual):\n",
    "    # false positive : predicted as positive but actuall zero sum(pred==1[actual == 0]) / sum(actual == 0)\n",
    "    accuracy = accuracy_score(actual,predicted)\n",
    "    f1 = f1_score(actual,predicted)\n",
    "    precision = precision_score(actual,predicted)\n",
    "#     fallout = np.sum(predicted[np.where(actual == 0)] == 1) / np.sum(actual == 0)\n",
    "    recall = recall_score(actual,predicted)\n",
    "    \n",
    "    return accuracy,f1,precision,recall\n",
    "\n",
    "def regression_metrics(predicted,actual):\n",
    "    return rmse(predicted,actual), prmse(predicted,actual), mad(predicted,actual), pmad(predicted,actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "hrundir = \"../models/run01_hz/\"\n",
    "orundir = \"../models/run01_gmean_4/\"\n",
    "srundir = \"../models/run01/\"\n",
    "\n",
    "nfolds = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_train = []\n",
    "summ_test = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** regression error summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fold in nfolds:\n",
    "\n",
    "    data = pickle.load(open(hrundir+str(fold)+\"/\"+\"data.pickle\",\"rb\"))\n",
    "    Xtrain = data['Xtrain']\n",
    "    Ytrain = data['Ytrain']\n",
    "    Xtest = data['Xtest']\n",
    "    Ytest = data['Ytest']\n",
    "\n",
    "    Ytrain_ind = (Ytrain > 0.05) * 1\n",
    "    Ytest_ind = (Ytest > 0.05) * 1\n",
    "\n",
    "    clf_results = pickle.load(open(hrundir+str(fold)+\"/\"+\"results_scgp.pickle\",\"rb\"))\n",
    "    zi_results  = pickle.load(open(hrundir+str(fold)+\"/\"+\"results_zi.pickle\",\"rb\"))\n",
    "    hdl_results = pickle.load(open(hrundir+str(fold)+\"/\"+\"results_hurdle.pickle\",\"rb\"))\n",
    "    svg_results = pickle.load(open(srundir+str(fold)+\"/\"+\"results_svgp.pickle\",\"rb\"))\n",
    "    oog_results = pickle.load(open(orundir+str(fold)+\"/\"+\"results_onoff.pickle\",\"rb\"))\n",
    "\n",
    "    train_clf_prob = clf_results['pred_train']['pfmean']\n",
    "    test_clf_prob  = clf_results['pred_test']['pfmean']\n",
    "    train_clf_indc = (train_clf_prob > 0.5) * 1.0\n",
    "    test_clf_indc  = (test_clf_prob > 0.5) * 1.0\n",
    "\n",
    "    train_zi_indc = (zi_results['pred_train_zi_indc'] > 1e-1) * 1\n",
    "    test_zi_indc  = (zi_results['pred_test_zi_indc'] > 1e-1) * 1\n",
    "    train_zi_reg = zi_results['pred_train_zi_indc']\n",
    "    test_zi_reg  = zi_results['pred_test_zi_indc']\n",
    "\n",
    "    train_00_indc = train_zi_reg*0.\n",
    "    test_00_indc  = test_zi_reg*0.\n",
    "    train_00_reg = train_zi_reg*0.\n",
    "    test_00_reg  = test_zi_reg*0.\n",
    "\n",
    "\n",
    "    train_hdl_indc  = (hdl_results['train_pred_hurdle_comb'] > 1e-1) * 1\n",
    "    test_hdl_indc  = (hdl_results['test_pred_hurdle_comb'] > 1e-1) * 1\n",
    "    train_hdl_reg  = hdl_results['train_pred_hurdle_comb']\n",
    "    test_hdl_reg  = hdl_results['test_pred_hurdle_comb']\n",
    "\n",
    "    train_svg_indc = (svg_results['pred_train']['fmean'] > 1e-1) * 1.0\n",
    "    test_svg_indc  = (svg_results['pred_test']['fmean'] > 1e-1) * 1.0\n",
    "    train_svg_reg = svg_results['pred_train']['fmean']\n",
    "    test_svg_reg  = svg_results['pred_test']['fmean']\n",
    "\n",
    "    train_oog_indc = (oog_results['pred_train']['gfmean'] > 1e-1) * 1.0\n",
    "    test_oog_indc  = (oog_results['pred_test']['gfmean'] > 1e-1) * 1.0\n",
    "    train_oog_reg = oog_results['pred_train']['gfmean']\n",
    "    test_oog_reg  = oog_results['pred_test']['gfmean']\n",
    "\n",
    "\n",
    "    ## CLASSIFICATION\n",
    "    train_00_accuracy,train_00_f1,train_00_precision, train_00_fallout = classification_metrics(train_00_indc,Ytrain_ind)\n",
    "    test_00_accuracy,test_00_f1,test_00_precision, test_00_fallout = classification_metrics(test_00_indc,Ytest_ind)\n",
    "\n",
    "    train_clf_accuracy,train_clf_f1,train_clf_precision, train_clf_fallout = classification_metrics(train_clf_indc,Ytrain_ind)\n",
    "    test_clf_accuracy,test_clf_f1,test_clf_precision, test_clf_fallout = classification_metrics(test_clf_indc,Ytest_ind)\n",
    "\n",
    "    train_zi_accuracy,train_zi_f1,train_zi_precision, train_zi_fallout = classification_metrics(train_zi_indc,Ytrain_ind)\n",
    "    test_zi_accuracy,test_zi_f1,test_zi_precision, test_zi_fallout = classification_metrics(test_zi_indc,Ytest_ind)\n",
    "\n",
    "    train_hdl_accuracy,train_hdl_f1,train_hdl_precision, train_hdl_fallout = classification_metrics(train_hdl_indc,Ytrain_ind)\n",
    "    test_hdl_accuracy,test_hdl_f1,test_hdl_precision, test_hdl_fallout = classification_metrics(test_hdl_indc,Ytest_ind)\n",
    "\n",
    "    train_svg_accuracy,train_svg_f1,train_svg_precision, train_svg_fallout = classification_metrics(train_svg_indc,Ytrain_ind)\n",
    "    test_svg_accuracy,test_svg_f1,test_svg_precision, test_svg_fallout = classification_metrics(test_svg_indc,Ytest_ind)\n",
    "\n",
    "    train_oog_accuracy,train_oog_f1,train_oog_precision, train_oog_fallout = classification_metrics(train_oog_indc,Ytrain_ind)\n",
    "    test_oog_accuracy,test_oog_f1,test_oog_precision, test_oog_fallout = classification_metrics(test_oog_indc,Ytest_ind)\n",
    "\n",
    "\n",
    "    ## REGRESSION\n",
    "    train_00_rmse,train_00_prmse,train_00_mad, train_00_pmad = regression_metrics(train_00_reg,Ytrain)\n",
    "    test_00_rmse,test_00_prmse,test_00_mad, test_00_pmad = regression_metrics(test_00_reg,Ytest)\n",
    "\n",
    "    train_zi_rmse,train_zi_prmse,train_zi_mad, train_zi_pmad = regression_metrics(train_zi_reg,Ytrain)\n",
    "    test_zi_rmse,test_zi_prmse,test_zi_mad, test_zi_pmad = regression_metrics(test_zi_reg,Ytest)\n",
    "\n",
    "    train_hdl_rmse,train_hdl_prmse,train_hdl_mad, train_hdl_pmad = regression_metrics(train_hdl_reg,Ytrain)\n",
    "    test_hdl_rmse,test_hdl_prmse,test_hdl_mad, test_hdl_pmad = regression_metrics(test_hdl_reg,Ytest)\n",
    "\n",
    "    train_svg_rmse,train_svg_prmse,train_svg_mad, train_svg_pmad = regression_metrics(train_svg_reg,Ytrain)\n",
    "    test_svg_rmse,test_svg_prmse,test_svg_mad, test_svg_pmad = regression_metrics(test_svg_reg,Ytest)\n",
    "\n",
    "    train_oog_rmse,train_oog_prmse,train_oog_mad, train_oog_pmad = regression_metrics(train_oog_reg,Ytrain)\n",
    "    test_oog_rmse,test_oog_prmse,test_oog_mad, test_oog_pmad = regression_metrics(test_oog_reg,Ytest)\n",
    "\n",
    "\n",
    "    ## return in a 2d array for train and test\n",
    "    train_array = np.array([[train_00_rmse,train_00_prmse,train_00_mad,train_00_pmad,train_00_accuracy,train_00_f1,train_00_precision, train_00_fallout],\n",
    "                            [np.nan,np.nan,np.nan,np.nan,train_clf_accuracy,train_clf_f1,train_clf_precision, train_clf_fallout],\n",
    "                            [train_zi_rmse,train_zi_prmse,train_zi_mad,train_zi_pmad,train_zi_accuracy,train_zi_f1,train_zi_precision, train_zi_fallout],\n",
    "                            [train_hdl_rmse,train_hdl_prmse,train_hdl_mad,train_hdl_pmad,train_hdl_accuracy,train_hdl_f1,train_hdl_precision, train_hdl_fallout],\n",
    "                            [train_svg_rmse,train_svg_prmse,train_svg_mad,train_svg_pmad,train_svg_accuracy,train_svg_f1,train_svg_precision, train_svg_fallout],\n",
    "                            [train_oog_rmse,train_oog_prmse,train_oog_mad,train_oog_pmad,train_oog_accuracy,train_oog_f1,train_oog_precision, train_oog_fallout]])\n",
    "\n",
    "    test_array = np.array([[test_00_rmse,test_00_prmse,test_00_mad,test_00_pmad,test_00_accuracy,test_00_f1,test_00_precision, test_00_fallout],\n",
    "                            [np.nan,np.nan,np.nan,np.nan,test_clf_accuracy,test_clf_f1,test_clf_precision, test_clf_fallout],\n",
    "                            [test_zi_rmse,test_zi_prmse,test_zi_mad,test_zi_pmad,test_zi_accuracy,test_zi_f1,test_zi_precision, test_zi_fallout],\n",
    "                            [test_hdl_rmse,test_hdl_prmse,test_hdl_mad,test_hdl_pmad,test_hdl_accuracy,test_hdl_f1,test_hdl_precision, test_hdl_fallout],\n",
    "                            [test_svg_rmse,test_svg_prmse,test_svg_mad,test_svg_pmad,test_svg_accuracy,test_svg_f1,test_svg_precision, test_svg_fallout],\n",
    "                            [test_oog_rmse,test_oog_prmse,test_oog_mad,test_oog_pmad,test_oog_accuracy,test_oog_f1,test_oog_precision, test_oog_fallout]])\n",
    "\n",
    "    summ_train.append(train_array)\n",
    "    summ_test.append(test_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_train = np.dstack(summ_train)\n",
    "summ_test = np.dstack(summ_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summ_train_mean = np.mean(summ_train,axis=2)\n",
    "summ_test_mean  = np.mean(summ_test,axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = ['model','rmse','prmse','mad','pmad','accuracy','f1','precis','recall']\n",
    "model_list  = ['baseline','clf','zi','hurlde','svgp','onoff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strr = ''\n",
    "for i in range(len(metrics_list)):\n",
    "    strr = strr + \"  \" + '{:<8s}'.format(metrics_list[i]) + \"  \"\n",
    "print(strr)\n",
    "\n",
    "for j in range(len(model_list)):\n",
    "    strr =  \"  \" + '{:<8s}'.format(model_list[j]) +  \"  \"\n",
    "    for i in np.arange(0,len(metrics_list)-1):\n",
    "        strr = strr + \"  \" + '{:<8.4f}'.format(summ_train_mean[j,i]) + \"  \"\n",
    "    print(strr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "strr = ''\n",
    "for i in range(len(metrics_list)):\n",
    "    strr = strr + \"  \" + '{:<8s}'.format(metrics_list[i]) + \"  \"\n",
    "print(strr)\n",
    "\n",
    "for j in range(len(model_list)):\n",
    "    strr =  \"  \" + '{:<8s}'.format(model_list[j]) +  \"  \"\n",
    "    for i in np.arange(0,len(metrics_list)-1):\n",
    "        strr = strr + \"  \" + '{:<8.4f}'.format(summ_test_mean[j,i]) + \"  \"\n",
    "    print(strr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf13]",
   "language": "python",
   "name": "conda-env-tf13-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
